 #**************************************************************
[DEFAULT]
LANGUAGE = English
LC = en
TREEBANK = DM
TB = dm
save_metadir = saves/AllData/${TREEBANK}/supsig/alldata-lemma
ElmoNetwork_dir = ${save_metadir}/${LANG}-${TREEBANK}/Elmo
TaggerNetwork_dir = ${save_metadir}/${LANG}-${TREEBANK}/Tagger
ParserNetwork_dir = ${save_metadir}/${LANG}-${TREEBANK}/Parser
GraphParserNetwork_dir = ${save_metadir}/${LANG}-${TREEBANK}/Parser
UnlabelGraphParserNetwork_dir = ${save_metadir}/${LANG}-${TREEBANK}/Parser
network_class = UnlabelGraphParserNetwork
save_dir = ${save_metadir}/${network_class}
train_conllus = data/AllData/${TREEBANK}/train.${LC}.${TB}.60.conllu
dev_conllus = data/AllData/${TREEBANK}/dev.${LC}.${TB}.conllu
test_conllus = data/AllData/${TREEBANK}/test.${LC}.*.${TB}.conllu

#***************************************************************
# Network
[Config]

[BaseNetwork]
n_passes = 0
max_steps = 500000
max_steps_without_improvement = 20000
print_every = 1000
save_model_after_improvement = True
save_model_after_training = False
parse_devset = False
switch_optimizers = True
# neural
l2_reg = 3e-9
output_keep_prob = .5
conv_keep_prob = .55
recur_keep_prob = .75
recur_include_prob = 1.
#hidden_keep_prob = 1.
n_layers = 3
first_layer_conv_width = 0
conv_width = 0
output_size = 100
recur_size = 600
output_func = identity
bidirectional = True
recur_cell = LSTM
decoder_recur_cell = LSTM
recur_func = tanh
cifg = False
# TODO try highway concatenation instead of addition
highway = False
highway_func = tanh
bilin = False
share_layer = False

[ElmoNetwork]
input_vocab_classes = FormSubtokenVocab
output_vocab_classes = FormTokenVocab
throughput_vocab_classes =
input_network_classes = None
#neural
recur_size = 500
n_layers = 2
n_samples = 1000

[TaggerNetwork]
input_vocab_classes = FormMultivocab
output_vocab_classes = UPOSTokenVocab:XPOSTokenVocab:UFeatsFeatureVocab
throughput_vocab_classes = LemmaTokenVocab:DepheadIndexVocab:DeprelTokenVocab
input_network_classes = None
#neural
n_layers = 2
recur_keep_prob = 0.5
recur_size = 200

[ParserNetwork]
input_vocab_classes = FormMultivocab:UPOSTokenVocab:XPOSTokenVocab:UFeatsFeatureVocab:LemmaTokenVocab
output_vocab_classes = DepheadIndexVocab:DeprelTokenVocab
throughput_vocab_classes =
input_network_classes = None
sum_pos = True
recur_size = 400

#[GraphParserNetwork]
#input_vocab_classes = FormMultivocab:XPOSTokenVocab:LemmaTokenVocab
#output_vocab_classes = SemheadGraphIndexVocab:SemrelGraphTokenVocab
#throughput_vocab_classes = None
#input_network_classes = None
#sum_pos = False
#recur_size = 600

[UnlabelGraphParserNetwork]
input_vocab_classes = FormMultivocab:XPOSTokenVocab:LemmaTokenVocab
output_vocab_classes = SemheadGraphIndexVocab:SemrelGraphTokenVocab
throughput_vocab_classes = None
input_network_classes = None
sum_pos = False
recur_size = 600
n_encoder_layers = 3
n_decoder_layers = 1
decoder_recur_size = 600
decoder_MLP_dep_layers = 1
mlp_hidden_sizes = 400
hidden_keep_prob = 0.75
restrict_loss_para = 0.
load_model = False
supervised = False
entropy_reg = 0.
unsup_strength = 0
label_strength = 1
[GraphOutputs]
[Flag]
portion = 1
fix_label_data = False
labeled_num = 3396
#**************************************************************
# CoNLLU fields
[CoNLLUVocab]

[FormVocab]
[LemmaVocab]
[UPOSVocab]
[XPOSVocab]
[UFeatsVocab]
[DepheadVocab]
[DeprelVocab]
[SemrelVocab]
[SemheadVocab]

#***************************************************************
# Datasets
[CoNLLUDataset]
max_buckets = 5
batch_size = 1500

[CoNLLUTrainset]
max_buckets = 30
batch_size = 1500

[CoNLLUDevset]

[CoNLLUTestset]

#**************************************************************
# Vocabulary types
[BaseVocab]

#===============================================================
# Numeric vocabs
[IndexVocab]
#neural
hidden_size = 600
hidden_keep_prob = .75
add_linear = True
n_layers = 1
hidden_func = identity
diagonal = False
linearize = False
distance = False

[IDIndexVocab]

[DepheadIndexVocab]

[SemheadGraphIndexVocab]

#===============================================================
# String Vocabs
[SetVocab]
cased = None
special_token_case = None
special_token_html = None
max_embed_count = 0
vocab_loadname =

[PretrainedVocab]
cased = False
special_token_case = upper
special_token_html = True
max_embed_count = 0
pretrained_file = None
name = None
vocab_loadname = ${save_metadir}/GloVe/glove.6B.100d.pkl
save_as_pickle = True
# neural
linear_size = 125
embed_keep_prob = .67

[FormPretrainedVocab]
pretrained_file = data/glove_vecs/glove.6B.100d.txt
name = glove

#===============================================================
# Token vocabs
[CountVocab]
cased = None
min_occur_count = None

[TokenVocab]
cased = True
special_token_case = upper
special_token_html = True
min_occur_count = 1
# neural
embed_size = 100
embed_keep_prob = .67
drop_func = unkout
hidden_size = 100
hidden_keep_prob = .67
n_layers = 1
add_linear = True
hidden_func = leaky_relu
diagonal = False

[FormTokenVocab]
cased = False
min_occur_count = 7
embed_size = 100

[LemmaTokenVocab]
cased = False
min_occur_count = 7
embed_size = 100
embed_keep_prob = .67

[UPOSTokenVocab]
special_token_html = False

[XPOSTokenVocab]
special_token_html = False
embed_size = 50
embed_keep_prob = .8

[DeprelTokenVocab]
special_token_case = lower
special_token_html = False
factorized = True
# neural
hidden_size = 200
diagonal = False
add_linear = True
loss_interpolation = .5

[SemrelGraphTokenVocab]
special_token_case = lower
special_token_html = False
factorized = True
# neural
add_linear = True
diagonal = True
loss_interpolation = .025
hidden_size = 600
hidden_func = identity
hidden_keep_prob = .67

#===============================================================
# Subtoken vocabs
[SubtokenVocab]
cased = False
special_token_case = upper
special_token_html = True
min_occur_count = 1
max_buckets = 3
token_vocab_loadname =
# neural
embed_size = 100
embed_keep_prob = 1.
conv_keep_prob = .67
recur_keep_prob = .67
recur_include_prob = 1.
output_keep_prob = .67
n_layers = 1
first_layer_conv_width = 1
conv_width = 0
recur_size = 400
bidirectional = False
recur_cell = LSTM
recur_func = tanh
output_func = identity
cifg = False
highway = False
highway_func = identity
bilin = False
squeeze_type = final_hidden
output_size = 100

[FormSubtokenVocab]
min_occur_count = 7

[LemmaSubtokenVocab]
min_occur_count = 7

#===============================================================
# Feature vocabs
[FeatureVocab]
vocab_loadname =
pad_str =
separator =
keyed = False
cased = True
min_occur_count = 0
max_embed_count = 0
# neural
hidden_keep_prob = .5
n_layers = 1
hidden_size = 100
hidden_func = leaky_relu
embed_keep_prob = .67
drop_func = unkout
embed_size = 50
diagonal = False
add_linear = True

[LemmaFeatureVocab]
separator = +
min_occur_count = 2

[XPOSFeatureVocab]
pad_str = -

[UFeatsFeatureVocab]
separator = |
keyed = True

#===============================================================
# Multivocabs
# TODO rework multivocabs
[Multivocab]
use_token_vocab = True
use_subtoken_vocab = False
use_pretrained_vocab = True
pretrained_files = None
names = None
# neural
combine_func = concat
embed_keep_prob = .8
drop_func = unkout

[FormMultivocab]
use_token_vocab = True
use_pretrained_vocab = True
use_subtoken_vocab = False

#***************************************************************
# Optimization
[Optimizer]
learning_rate = .005
decay_rate = 0
clip = 5.
mu = 0
nu = .95
epsilon = 1e-12
gamma = 0

[AMSGradOptimizer]