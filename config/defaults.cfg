#**************************************************************
[DEFAULT]
LANG = English
LC = en
TREEBANK = EWT
TB = ewt
save_metadir = saves/CoNLL18
data_metadir = data/CoNLL18
ElmoNetwork_dir = ${save_metadir}/${LANG}-${TREEBANK}/Elmo
TaggerNetwork_dir = ${save_metadir}/${LANG}-${TREEBANK}/Tagger
ParserNetwork_dir = ${save_metadir}/${LANG}-${TREEBANK}/Parser
network_class = 
save_dir = ${save_metadir}/${LANG}-${TREEBANK}/${network_class}
train_conllus = ${data_metadir}/UD_${LANG}-${TREEBANK}/${LC}_${TB}-ud-train.conllu
dev_conllus = ${data_metadir}/UD_${LANG}-${TREEBANK}/${LC}_${TB}-ud-dev.conllu
test_conllus = ${dev_conllus}

#***************************************************************
# Network
[Config]

[BaseNetwork]
n_passes = 0
max_steps = 30000
max_steps_without_improvement = 3000
print_every = 100
save_model_after_improvement = True
save_model_after_training = False
parse_devset = True
switch_optimizers = True
# neural
l2_reg = 0
output_keep_prob = .5
conv_keep_prob = .5
recur_keep_prob = .75
recur_include_prob = 1.
#hidden_keep_prob = .67
n_layers = 3
first_layer_conv_width = 0
conv_width = 0
output_size = 100
recur_size = 200
output_func = identity
bidirectional = True
recur_cell = LSTM
recur_func = tanh
cifg = False
# TODO try highway concatenation instead of addition
highway = True
highway_func = tanh
bilin = True
share_layer = False

[ElmoNetwork]
input_vocab_classes = FormSubtokenVocab
output_vocab_classes = FormTokenVocab
throughput_vocab_classes = 
input_network_classes = None
#neural
recur_size = 500
n_layers = 2
n_samples = 1000

[TaggerNetwork]
input_vocab_classes = FormMultivocab
output_vocab_classes = UPOSTokenVocab:XPOSTokenVocab:UFeatsFeatureVocab
throughput_vocab_classes = LemmaTokenVocab:DepheadIndexVocab:DeprelTokenVocab
input_network_classes = None
#neural
n_layers = 2
recur_keep_prob = .5
recur_size = 200

[ParserNetwork]
input_vocab_classes = FormMultivocab:UPOSTokenVocab:XPOSTokenVocab:UFeatsFeatureVocab:LemmaTokenVocab
output_vocab_classes = DepheadIndexVocab:DeprelTokenVocab
throughput_vocab_classes = 
input_network_classes = None
sum_pos = True
recur_size = 400

[GraphParserNetwork]

[GraphDecoderNetwork]

[UnlabelGraphParserNetwork]

[GraphOutputs]

#**************************************************************
# CoNLLU fields
[CoNLLUVocab]

[FormVocab]
[LemmaVocab]
[UPOSVocab]
[XPOSVocab]
[UFeatsVocab]
[DepheadVocab]
[DeprelVocab]
[SemrelVocab]
[SemheadVocab]

#***************************************************************
# Datasets
[CoNLLUDataset]
max_buckets = 5
batch_size = 50000

[CoNLLUTrainset]
max_buckets = 15
batch_size = 5000

[CoNLLUDevset]

[CoNLLUTestset]

#**************************************************************
# Vocabulary types
[BaseVocab]

#===============================================================
# Numeric vocabs
[IndexVocab]
#neural
hidden_size = 400
hidden_keep_prob = .5
add_linear = True
n_layers = 1
hidden_func = leaky_relu
diagonal = False
linearize = True
distance = True

[IDIndexVocab]

[DepheadIndexVocab]

[SemheadGraphIndexVocab]

#===============================================================
# String Vocabs
[SetVocab]
cased = None
special_token_case = None
special_token_html = None
max_embed_count = 0
vocab_loadname = 

[PretrainedVocab]
cased = False
special_token_case = upper
special_token_html = True
max_embed_count = 0
save_as_pickle = True
vocab_loadname = None
pretrained_file = None
name = None
# neural
linear_size = 125
embed_keep_prob = .67

[FormPretrainedVocab]
vocab_loadname = ${save_metadir}/${LANG}/${LC}.vectors.pkl
pretrained_file = data/word2vec/${LANG}/${LC}.vectors.xz
name = word2vec

#===============================================================
# Token vocabs
[CountVocab]
cased = None
min_occur_count = None

[TokenVocab]
cased = True
special_token_case = upper
special_token_html = True
min_occur_count = 1
# neural
embed_size = 50
embed_keep_prob = .67
drop_func = unkout
hidden_size = 400
hidden_keep_prob = .5
n_layers = 1
add_linear = True
hidden_func = leaky_relu
diagonal = False

[FormTokenVocab]
cased = False
min_occur_count = 7
#embed_size = ${SubtokenVocab:output_size}
embed_size = 75

[LemmaTokenVocab]
cased = False
min_occur_count = 7
embed_size = 75
embed_keep_prob = .67

[UPOSTokenVocab]
special_token_html = False

[XPOSTokenVocab]
special_token_html = False

[DeprelTokenVocab]
special_token_case = lower
special_token_html = False
factorized = True
# neural
hidden_size = 400
diagonal = False
add_linear = True
loss_interpolation = .5

[SemrelGraphTokenVocab]
special_token_case = lower
special_token_html = False
factorized = True
# neural
add_linear = True
loss_interpolation = .033

#===============================================================
# Subtoken vocabs
[SubtokenVocab]
cased = True
special_token_case = upper
special_token_html = True
min_occur_count = 1
max_buckets = 2
token_vocab_loadname = 
# neural
embed_size = 100
embed_keep_prob = .9
conv_keep_prob = .5
recur_keep_prob = .75
recur_include_prob = 1.
output_keep_prob = .5
n_layers = 1
first_layer_conv_width = 0
conv_width = 0
recur_size = 400
bidirectional = False
recur_cell = LSTM
recur_func = tanh
output_func = identity
cifg = False
highway = False
highway_func = identity
bilin = False
squeeze_type = linear_attention
output_size = 125

[FormSubtokenVocab]
min_occur_count = 7

[LemmaSubtokenVocab]
min_occur_count = 7

#===============================================================
# Feature vocabs
[FeatureVocab]
vocab_loadname = 
pad_str = 
separator = 
keyed = False
cased = True
min_occur_count = 0
max_embed_count = 0
# neural
hidden_keep_prob = .5
n_layers = 1
hidden_size = 100
hidden_func = leaky_relu
embed_keep_prob = .67
drop_func = unkout
embed_size = 50
diagonal = False
add_linear = True

[LemmaFeatureVocab]
separator = +
min_occur_count = 2

[XPOSFeatureVocab]
pad_str = -

[UFeatsFeatureVocab]
separator = |
keyed = True

#===============================================================
# Multivocabs
# TODO rework multivocabs
[Multivocab]
use_token_vocab = True
use_subtoken_vocab = False
use_pretrained_vocab = True
pretrained_files = None
names = None
# neural
combine_func = concat
embed_keep_prob = .67
drop_func = unkout

[FormMultivocab]
use_token_vocab = True
use_pretrained_vocab = True
use_subtoken_vocab = True

#***************************************************************
# Optimization
[Optimizer]
learning_rate = .003
decay_rate = 0
clip = 1.
mu = .9
nu = .95
epsilon = 1e-12
gamma = 0

[AMSGradOptimizer]
